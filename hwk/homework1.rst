============================================================
Homework 1 : Galen Collins
============================================================

------------------------------------------------------------
1.a : Prove that all nonempty subsets of f are frequent
------------------------------------------------------------

We can prove that this is true using the following lemma:

  1. Let I = { i1, i2, .., im } be a set of items
  2. Let T = { t1, t2, .., tn } be a set of transactions where
     each transaction ti is a set of items s.t. ti E I
  3. Given a frequent itemset f = { i1, i2, .., ik } where ij E I
  4. A frequent itemset is one s.t. f.count / n >= minsup where
     
     * f.count is the number of transactions in T that contain f
     * n is the total number of transactions in T
     * minsup is the minimum percentage of transactions in T that
       must contain the given itemset for said itemset to be considered
       frequent.

  5. Since f.count transactions in T contain f, it follows that any
     nonempty subset of f must also be contained in T.
  6. This means that the subset fs of f must have at least f.count support
  7. Therefore, fs must also be frequent
  8. So all nonempty subsets of f must also be frequent


------------------------------------------------------------
1.b : Prove that all nonempty subsets of s have >= s.support
------------------------------------------------------------

We can prove that this is true using the following lemma:

  1. Let I = { i1, i2, .., im } be a set of items
  2. Let T = { t1, t2, .., tn } be a set of transactions where
     each transaction ti is a set of items s.t. ti E I
  3. Given an itemset s = { i1, i2, .., ik } where ij E I
  4. The support of s is s.count / n where:
     
     * s.count is the number of transactions in T that contain f
     * n is the total number of transactions in T.

  5. Since s.count transactions in T contain s, it follows that any nonempty subset of s must also be contained in T.
  7. This means that the count of subset s' of s must bet at least s.count if not more
  8. So all nonempty subsets of s must have support >= the support of s

------------------------------------------------------------
2.a : Discover all the frequent itemsets
------------------------------------------------------------

The following are the results generated by performing the Apriori
algorithm on the input datasets::

    N       = 5 // dataset.size
    minsup  = 60%
    minconf = 80%
    
    C1 = { (A):1, (C):2, (D):1, (E):4, (I):1, (K):5, (M):3, (N):2, (O):3, (U):1, (Y):3 }
    F1 = { E[4/5], K[5/5], M[3/5], O[3/5], Y[3/4] } 
    C2 = { (E,K):4, (E,M):2, (E,O):3, (E,Y):2, (K,M):3, (K,O):3, (K,Y):3, (M,O):1, (M,Y):2, (O,Y):2 }
    F2 = { (E,K)[4/5], (E,O)[3/5], (K,M)[3/5], (K,O)[3/5], (K,Y)[3/5] }
    C3 = { (E,K,O):3 }
    F3 = { (E,K,O)[3/5] }

As the minimum support required was 60%, all candidates that had support
less than this were pruned from the frequent itemset N.

Note that in candidate3 we pruned the rules (K,M,O) (K,M,Y) (K,O,Y) as they
contained infrequent subsets (KOY => OY:[2/5] < 60%)

------------------------------------------------------------
2.b : Generate strong rules for the frequent items
------------------------------------------------------------

Since the constraint was for the rules to be of::

    buys(X, item1) and buys(X, item2) => buys(X, item3)

Only the frequent itemset 3 will be considered for rule generation::

    { 0, K } => { E } [3/3]
    { 0, E } => { K } [3/3]
    { K, E } => { O } [3/4] // does not have 80% confidence

Therfore the final discovered rules are as follows::

    { 0, K } => { E } s[3/5] c[3/3]
    { 0, E } => { K } s[3/5] c[3/3]

------------------------------------------------------------
3 : Describe the major steps of decision tree learning
------------------------------------------------------------

The first step, if it is needed, is to convert the input data
attirbutes to categorical form.  This can be performed on discrete
data by using range bins (one possible example).  Next, using divide
and conquer, recursively partition the dataset based on a given attribute
that produces the greatest information gain (for that partition). Continue
this until one of the following conditions occurs:

  1. All examples for the current node belong to the same class (pure)
  2. There are no more remaining attributes to partition with
  3. There are no examples left to work with
  4. Optionally: if further processing does not produce acceptable information
     gain (say by some threshold), the remaing node processing may be abandoned
     (pre pruning).

The tree may then be pruned to simplify the model or to give a better image of
how the classifier works. New examples can now be classified by running them
through the tree and then applying the classification of the leaf node that the
finish on.

------------------------------------------------------------
4 : Should you preprune or postprune a decision tree
------------------------------------------------------------

Given the choice, one should always postprune a decision tree. The reason for
this is that it gives you greater control over what information you want to
remove.  If you preprune you are forced to remove an entire path below a node
(and you may not know how a node will finish processing).

However, if the node is post processed, you are able to remove a single path
throughout the tree while not eliminating a nodes attribute decision.


------------------------------------------------------------
5.a : Estimate the conditional probabilities
------------------------------------------------------------

Using the standard _ formula, we come up with the following conditional
probabilities.  However, we notice that there is one probability that has
resolved to 0 and it is used in the second part of this question. In order
to prevent this value from zeroing the total results, we add a small correction
to all values using the corrected estimate formula::

    (A = a|C = c) = (ni + ~)/(nc + mi)

    ~  = 1 / # of examples            = 0.1
    mi = the number of attributes * ~ = 0.2

    P(A=0|+) = (2 + ~)/(5 + mi) = 2.1/5.2    P(A=0|-) = (3 + ~)/(5 + mi) = 3.1/5.2
    P(A=1|+) = (3 + ~)/(5 + mi) = 3.1/5.2    P(A=1|-) = (2 + ~)/(5 + mi) = 2.1/5.2
                                                              
    P(B=0|+) = (4 + ~)/(5 + mi) = 4.1/5.2    P(B=0|-) = (3 + ~)/(5 + mi) = 3.1/5.2
    P(B=1|+) = (1 + ~)/(5 + mi) = 1.1/5.2    P(B=1|-) = (2 + ~)/(5 + mi) = 2.1/5.2
                                                              
    P(C=0|+) = (1 + ~)/(5 + mi) = 1.1/5.2    P(C=0|-) = (0 + ~)/(5 + mi) = 0.1/5.2
    P(C=1|+) = (4 + ~)/(5 + mi) = 4.1/5.2    P(C=1|-) = (5 + ~)/(5 + mi) = 5.1/5.2

------------------------------------------------------------
5.b : Predict the class label with the probabilities
------------------------------------------------------------

Given the test sample (A = 0, B = 1, and C = 0), we can apply the naive
Bayes approach to determing the correct label for the sample. First we
need the prior probabilities of each class (+/-)::

    P(C) = # of samples of class C / # of examples
    P(C = +) = 5/10 = 1/2    P(C = -) = 5/10 = 1/2

Then using the formula to solve P(C = c)::

    Pr(C = +) = (1/2)(21/52 * 11/52 * 11/52) = 0.009
    Pr(C = -) = (1/2)(31/52 * 21/52 *  1/52) = 0.002 
    0.009 > 0.002

Therefore the correct classifier for this sample is (+)

--------------------------------------------------------------
6.a : Design a kernel function and two SVM classifiers in WEKA
--------------------------------------------------------------

Given the input data set (as follows), I played with the SVM classifiers
in WEKA (results attached on the back). For the hard margin I achieved a pretty
low correct classifcation rate across the board, 44%, which was less usefull than
a random classifier.

However, by switching to a soft margin with the normalized polynomial kernal (along
with a little bit of parameter trweaking) I was able to increase my positive classification
rate to 55%, which while still horrible, beats a random coin toss and was the best I could
produce for this dataset::

    Input | Sign
    ------------
    -3    | +
     1    | +
     0    | -
     2    | -
     3    | -
     6    | +
     9    | +
    14    | -
    20    | -

------------------------------------------------------------
6.b : Describe what the C parameter does to SVM
------------------------------------------------------------

The C parameter is the complexity constant which when dealing with
a soft margin controls the error rate of the slack variables.  A larger
C will penaliez slack variables more while decreasing it will allow more
slack in the margin.

------------------------------------------------------------
7.a : General probabilities question
------------------------------------------------------------

Given the following::

    Coin  = {P(heads), P(Tails)}

    Cfair = {0.50, 0.50}    P(Cf) = 0.20
    Cload = {0.75, 0.25}    P(Cl) = 0.80

    D = <H, T, T, H, T, H, H, T, T, H>

We can determine which coin the dealer is using by first computing
the probability of each coin after each flip in the dataset. We can
achieve this using the following::

    P(Cf|d1 = H) = (~)P(H|Cf)P(Cf) = (~)(0.50)(0.2)
    P(Cl|d1 = H) = (~)P(H|Cl)P(Cl) = (~)(0.75)(0.8)

We can solve for (~) by using the fact that the sum of the two probabilities
must equal 1, therefore::

    (~)(0.50)(0.2) + (~)(0.75)(0.8) = 1
    (~)(0.70) = 1
    (~) = 1.429

0.07145
Plugging this in for the d1 sample gives us::

    P(Cf|d1 = H) = (1.429)(0.50)(0.2) = 0.1429
    P(Cl|d1 = H) = (1.429)(0.75)(0.8) = 0.8574

We can then solve for d2 by using the d1 prior, however, since we already found
the probability for P(C|d1) = P(d1|C)P(C), we can simply plug it in::

    P(Cf|d2 = T) = (~)P(T|Cf)(H|Cf)P(Cf) = (~)(0.50)(0.1429)
    P(Cl|d2 = T) = (~)P(T|Cl)(H|Cl)P(Cl) = (~)(0.25)(0.8574)

And solving for (~) = 3.5 gives us the following::

    P(Cf|d2 = T) = = (3.5)(0.50)(0.1429) = 0.250
    P(Cl|d2 = T) = = (3.5)(0.25)(0.8574) = 0.765

This is continued through the entire data set until we arrive with the following
results (calculated in excel)::

    I     d    P(Cf|d)   P(Cl|d)   (~)
    --------------------------------------
     1    H    0.1429    0.8571    1.4286
     2    T    0.2500    0.7500    3.5000
     3    T    0.4000    0.6000    3.2000
     4    H    0.3077    0.6923    1.5385
     5    T    0.4706    0.5294    3.0588
     6    H    0.3721    0.6279    1.5814
     7    H    0.2832    0.7168    1.5221
     8    T    0.4414    0.5586    3.1172
     9    T    0.6124    0.3876    2.7751
    10    H    0.5130    0.4870    1.6754

The data seems to give a trend of the fair coin rising in probability while
the probability of the loaded coin being used seems to diminish with time. With
this in mind and the fact that the last probability calculation shows the fair
coin edging out just slightly, it can be assumed that the fair coin is the one
that is currently in play.

------------------------------------------------------------
7.b : What coin will be tossed next
------------------------------------------------------------

In order to find out what coin will be tossed next, we simply
calculate the probabilities of each toss (H/T) and choose the
more probable result::

    P(H|D) = sum(P(H|Cx) * P(Cx|D)) = P(H|Cf) * P(Cf|D) + P(H|Cl) * P(Cl|D)
    P(T|D) = 1 - P(H|D)

    P(H|D) = (0.5)*(0.513) + (0.75)(0.487) = 0.6218 => 62.2%
    P(T|D) = 1 - P(H|D)                    = 0.3782 => 37.8%

Since the probability of heads being thrown is much larger than that of tails,
the smart move would be to bet on heads.
